---
title: "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
collection: publications
permalink: /publication/2021-01-01-pile
excerpt: '<p>[<a href="https://arxiv.org/abs/2101.00027" style="color:#51ADC8;">Paper</a>] [<a href="https://pile.eleuther.ai/" style="color:#51ADC8;">Website</a>] - <a href="/publication/2021-01-01-pile" style="color:#51ADC8;">Abstract</a><br /><span style="font-family:Courier New">Citation</span>: Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy. <u>The Pile: An 800GB Dataset of Diverse Text for Language Modeling</u>. <i>Preprint, 2021.</i></p>'
date: 2021-01-01
venue: 'Preprint'
---

Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present <i>>the Pile</i>: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction. 

[<a href="https://arxiv.org/abs/2101.00027" style="color:#51ADC8;">Paper</a>]
[<a href="https://pile.eleuther.ai/" style="color:#51ADC8;">Website</a>]

<span style="font-family:Courier New">Citation</span>: Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy. <u>The Pile: An 800GB Dataset of Diverse Text for Language Modeling</u>. <i>Preprint, 2021.</i> 