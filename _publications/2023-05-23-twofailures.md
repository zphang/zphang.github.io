---
title: "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs"
collection: publications
permalink: /publication/2023-05-23-twofailures
excerpt: '<p>[<a href="https://arxiv.org/abs/2305.14279" style="color:#51ADC8;">Paper</a>] - <a href="/publication/2023-05-23-twofailures" style="color:#51ADC8;">Abstract</a><br /><span style="font-family:Courier New">Citation</span>: Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R. Bowman, Kyunghyun Cho <u>Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs</u>. <i>TMLR 2024.</i></p>'
date: 2023-05-23
venue: 'TMLR 2024'
---

Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.
[<a href="https://arxiv.org/abs/2305.14279" style="color:#51ADC8;">Paper</a>]

<span style="font-family:Courier New">Citation</span>: Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R. Bowman, Kyunghyun Cho <u>Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs</u>. <i>TMLR 2024.</i> 