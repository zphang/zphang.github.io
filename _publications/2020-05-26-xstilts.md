---
title: "English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too"
collection: publications
permalink: /publication/2020-05-26-xstilts
excerpt: '<p>[<a href="https://arxiv.org/abs/2005.13013" style="color:#51ADC8;">Paper</a>] - <a href="/publication/2020-05-26-xstilts" style="color:#51ADC8;">Abstract</a><br /><span style="font-family:Courier New">Citation</span>: Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, Samuel R. Bowman. <u>English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too</u>. <i>Proceedings of AACL, 2020.</i></p>'
date: 2020-05-26
venue: 'Proceedings of AACL, 2020'
---

Intermediate-task training---fine-tuning a pretrained model on an intermediate task before fine-tuning again on the target task---often improves model performance substantially on language understanding tasks in monolingual English settings. We investigate whether English intermediate-task training is still helpful on non-English target tasks. Using nine intermediate language-understanding tasks, we evaluate intermediate-task transfer in a zero-shot cross-lingual setting on the XTREME benchmark. We see large improvements from intermediate training on the BUCC and Tatoeba sentence retrieval tasks and moderate improvements on question-answering target tasks. MNLI, SQuAD and HellaSwag achieve the best overall results as intermediate tasks, while multi-task intermediate offers small additional improvements. Using our best intermediate-task models for each target task, we obtain a 5.4 point improvement over XLM-R Large on the XTREME benchmark, setting the state of the art as of June 2020. We also investigate continuing multilingual MLM during intermediate-task training and using machine-translated intermediate-task data, but neither consistently outperforms simply performing English intermediate-task training. 

[<a href="https://arxiv.org/abs/2005.13013">Paper</a>]

<span style="font-family:Courier New">Citation</span>: Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, Samuel R. Bowman. <u>English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too</u>. <i>Proceedings of AACL, 2020.</i> 