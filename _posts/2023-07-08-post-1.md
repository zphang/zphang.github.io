---
title: 'Should we consider LoRA an 'adapter'?'
date: 2023-07-08
permalink: /posts/2023/07/post1/
tags:
  - peft
  - nitpicking
---

*Is LoRA an adapter? Historically no. But should we call it one?*

Introduction
======

This is a post purely about naming conventions, but it might be a worthwhile resource and a useful discussion to have. Especially with the large wave of newcomers to the field of NLP after the release of ChatGPT, I think that it is worth going to extra mile to ensure we get naming conventions right.

This impetus for this post is a discussion I had with [Carson Poole](https://twitter.com/carsonpoole) over on the [EleutherAI Discord](https://discord.gg/zBGx3azzUn), but I figured that it was useful to write all this down for future reference.

Background
======

[LoRA](https://arxiv.org/abs/2106.09685) is a parameter-efficient fine-tuning (PEFT) method published in 2021. Although it is a fairly general PEFT method, it was initially applied to BERT-type and GPT-2 models. It saw an immense resurgence in popularity after both its application in Stable Diffusion model in 2022, and a second wave of interest following the dissemination of LLaMA models and the wave of open-source activity on fine-tuning LLaMA.

Test LaTeX $hello$

*(Side note: the initial version of LoRA only fine-tuned the Q and V linear mappings (with some additional experiments on all four attention linear layers). However, later work has found that LoRA truly shines when you apply it to all linear and even embedding layers in a Transformer (e.g. in [QLoRA](https://arxiv.org/abs/2305.14314)).)*


