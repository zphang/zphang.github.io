---
title: 'Should we consider LoRA an 'adapter'?'
date: 2023-07-08
permalink: /posts/2023/07/post1/
tags:
  - peft
  - nitpicking
---

*Is LoRA an adapter? Historically no. But should we call it one?*

Introduction
======

This is a post purely about naming conventions, but it might be a worthwhile resource and a useful discussion to have. Especially with the large wave of newcomers to the field of NLP after the release of ChatGPT, I think that it is worth going to extra mile to ensure we get naming conventions right.

This impetus for this post is a discussion I had with [Carson Poole](https://twitter.com/carsonpoole) over on the [EleutherAI Discord](https://discord.gg/zBGx3azzUn), and I figured that it was useful to write all this down for future reference. Many of the points here were raised in our discussion by either myself or Carson.

Background
======

[LoRA](https://arxiv.org/abs/2106.09685) (Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) method published in 2021, which involves learning a low-rank additive modification to linear layers in a Transformer.. Although it is a fairly general PEFT method, it was initially applied to BERT-type and GPT-2 models. It saw an immense resurgence in popularity after both its application in Stable Diffusion model in 2022, and a second wave of interest following the dissemination of LLaMA models and the wave of open-source activity on fine-tuning LLaMA.

*(Side note: the initial version of LoRA only fine-tuned the Q and V linear mappings (with some additional experiments on all four attention linear layers). However, later work has found that LoRA truly shines when you apply it to all linear and even embedding layers in a Transformer (e.g. in [QLoRA](https://arxiv.org/abs/2305.14314)).)*

The original [Adapter](https://arxiv.org/abs/1902.00751) was introduced in 2019. Initially applied to BERT, it involves learning a residual low-rank MLP to a frozen Transformer model. Since the release of [subsequent Adapter variants](https://arxiv.org/abs/2005.00247), it has sometimes been retroactively referred to as a Houlsby Adapter, after its first author.

Why LoRA is not an Adapter.
======

Simply put, LoRA and Adapters are separate methods.

I believe a big reason for why LoRA have been called "adapters" informally is because of the similarity of the name: it is easy to confuse "Low Rank Adaptation" for "Low Rank Adapters", as seen [here](https://arxiv.org/abs/2305.14314) and [here](https://arxiv.org/abs/2210.07558). However, the original paper never once refers to LoRA as an Adapter, only as an "adaptation method". Given that there is another method named Adapters, and the reference to LoRA as an Adapter appears to arise from a misconception, I think we should undo this mistake, put our foot down, and say that LoRA is not an Adapter. 
